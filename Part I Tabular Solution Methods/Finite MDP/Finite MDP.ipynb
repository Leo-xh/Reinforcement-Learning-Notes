{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite MDP\n",
    "\n",
    "#### MDP\n",
    "\n",
    "MDP property: The state must inlcude information about all aspects of the past agent-environment interaction that make a difference for the future.\n",
    "\n",
    "MDP does not mean that the action decision is irrelative with the previous time, but the state is defined to have MDP property, by Markov observation, even non-Markov ones.\n",
    "\n",
    "#### Episode\n",
    "\n",
    "The distinction between continous and episodic task is that episodic task will restart the job periodically.\n",
    "\n",
    "#### Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "The method is to add absorbing state with reward 0 after the terminating state.\n",
    "\n",
    "#### Bellman Equation\n",
    "\n",
    "Dynamic programming equation in dynamic programming.\n",
    "\n",
    "#### Bellman Optimal  Equation\n",
    "$$\n",
    "    v_*(s) \\doteq \\max_\\pi v_\\pi(s) \\quad \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "$$\n",
    "    q_*(s, a) = \\max_\\pi q_\\pi (s, a) \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\n",
    "$$\n",
    "$$\n",
    "    q_*(s, a) = E{} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_{t} = s, A_t = a]\n",
    "$$\n",
    "$$\n",
    "    v_*(s) = \\max_a \\sum_{s', r} p(s', r|s, a) [r + \\gamma v_*(s')]\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q_*(s, a)  &= \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'}q_*(S_{t+1}, a') | S_t=s, A_t = a]\\\\\n",
    "           &= \\sum_{s', r} p(s', r| s, a ) [r + \\gamma \\max_{a'}q_*(s', a')]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "*Backup diagrams can be used to memorize the Bellman equations.*\n",
    "\n",
    "*Choose optimal function is to transfer a expectation operation to a max operation, by making the optimal policy select the best action with probability 1.*\n",
    "#### Optimal and Approximation\n",
    "\n",
    "A feature of reinforcement learning is that it will pay more effort to the cases that happen frequently.\n",
    "\n",
    "#### Extension of MDP\n",
    "\n",
    "+ POMDP\n",
    "Add observations and observation functions$\\mathbb{S}$ to the MDP.\n",
    "\n",
    "> A history $H_t$ is a sequence of actions, observations and rewards,\n",
    "A belief state b(h) is a probability distribution over states,\n",
    "conditioned on the history h."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
