{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite MDP Solutions\n",
    "\n",
    "#### 3.1\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. A robot that is attempting to reach a object, the actions can be the angle selected, states are the current place, rewards is +1 if the object is touch, else 0.\n",
    "2. Pacman game, actions can be the movement directions, states are the places and the bullet places, rewards are positive scores getted by eating bullets.\n",
    "3. Puzzle game, actions can be a mapping from patches to places, states are the whole current map, rewards are the positive scores if the place is right.\n",
    "\n",
    "#### 3.2\n",
    "\n",
    "**Answer**\n",
    "\n",
    "No, MDP framework is not adequate to the problems that the decision depends not only on current but also the previous state, such as the prediction of weather.\n",
    "\n",
    "#### 3.3\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. It depends on the tasks.\n",
    "2. The evaluation can focus on how it is close to the decision part of the problem.\n",
    "3. Actions have to be things that the agent can actually control.\n",
    "\n",
    "#### 3.4\n",
    "\n",
    "**Answer**\n",
    "\n",
    "|s|a|s'|r|p(s',r &#124; s,a)|\n",
    "|:-:|:-:|:-:|:-:|:-|\n",
    "|high|search|high|$r_{search}$|$\\alpha$\n",
    "|high|search|low|$r_{search}$|$1 - \\alpha$\n",
    "|high|wait|high|$r_{wait}$|$1$\n",
    "|low|recharge|high|$0$|$1$\n",
    "|low|search|high|$-3$|$1 - \\beta$ \n",
    "|low|search|low|$r_{search}$|$\\beta$\n",
    "|low|wait|low|$r_{wait}$|$1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    \\sum_{s'\\in S^{+}}\\sum_{r \\in R}p(s',r|s,a)=1\n",
    "$$\n",
    "\n",
    "#### 3.6\n",
    "\n",
    "**Answer**\n",
    "\n",
    "+ Episodic case:\n",
    "\n",
    "    $$\n",
    "        G_t = -{\\gamma}^{T-t}\n",
    "    $$\n",
    "+ Continous case:\n",
    "    \n",
    "    $$\n",
    "        G_t = \\sum_{k \\in K} {\\gamma}^{k-t}\n",
    "    $$\n",
    "\n",
    "#### 3.7\n",
    "\n",
    "**Answer**\n",
    "\n",
    "+ Firstly, under most strategies the reward shown in **3.7** will be 1, which encourages the agent to move randomly.\n",
    "+ Secondly, in some cases, the agent will not escape from the maze.\n",
    "+ Thirdly, the real aim is exactly to make the agent escape from the maze as quickly as possible.\n",
    "\n",
    "#### 3.8\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    G_0 = 2\n",
    "    G_1 = 6\\\\\n",
    "    G_2 = 8\\\\\n",
    "    G_3 = 4\\\\\n",
    "    G_4 = 2\\\\\n",
    "    G_5 = 0\n",
    "$$\n",
    "\n",
    "#### 3.9\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    G_1 = \\sum_{t=2}^{\\infty}\\gamma^{(t-2)}\\times 7 = \\frac{7\\gamma}{1-\\gamma}\\\\\n",
    "    G_0 = R_1+\\sum_{t=2}^{\\infty}\\gamma^{(t-2)}\\times 7 = 2+\\frac{7\\gamma}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "#### 3.10\n",
    "\n",
    "**Answer**\n",
    "\n",
    "By the formulation of the sum of geometric sequence, $G_t = \\sum_{k=0}^{\\infty}y^k=\\lim_{N\\to\\infty}\\frac{1-\\gamma^{N}}{1-\\gamma}=\\frac{1}{1-\\gamma}$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    E_{\\pi}[R_{t+1}|S_t=s]=\\sum_{a}\\pi(a|s)\\sum_{s',r}rp(s',r|s,a)\n",
    "$$\n",
    "\n",
    "#### 3.12\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    v_{\\pi}(s) = \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "#### 3.13\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    q_{\\pi}(s,a) = \\sum_{s',r}p(s',r|s,a)[\\gamma v_{\\pi}(s')+r]\n",
    "$$\n",
    "\n",
    "#### 3.14\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    0.7 \\approx \\frac{(2.3+0.4-0.4+0.7)*0.9}{4}=0.675\n",
    "$$\n",
    "\n",
    "#### 3.15\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    v_c = \\frac{c}{1-\\gamma}\n",
    "$$\n",
    "Since $v_c$ is added to every state's value, the relative values are unchanged.\n",
    "\n",
    "#### 3.16\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    v_c = c\\frac{1-{\\gamma}^T}{1-\\gamma}\n",
    "$$\n",
    "So the change depends on the termination time of every episode.\n",
    "\n",
    "#### 3.17\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    q_{\\pi}(s,a) = \\sum_{s',r}p(s',r|s,a)(r+\\gamma\\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a'))\n",
    "$$\n",
    "\n",
    "#### 3.18\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    v_\\pi(s) & = E[q_\\pi(S_t, A_t) |S_t = s, A_t = a] \\\\ \n",
    "          & = \\sum_a \\pi(a|s) q_\\pi(s, a)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### 3.19\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q_\\pi(s, a) & = E[R_{t+1} + v_\\pi(s') |S_t = s, A_t = a] \\\\ \n",
    "          & = \\sum_{s', r} p(s', r|s, a) [r + v_\\pi(s')]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.20\n",
    "**Answer**\n",
    "\n",
    "It is apparent that the optimal policy is to putt in the green area, and to drive off the green area.\n",
    "\n",
    "So the optimal state-value function is using the function of putt when in the green area, and the function of driver when off the green area.\n",
    "\n",
    "#### 3.21\n",
    "**Answer**\n",
    "\n",
    "As described in **3.20**.\n",
    "\n",
    "#### 3.22\n",
    "**Answer**\n",
    "\n",
    "+ $\\gamma$ is 0: $\\pi_{left}$ is optimal.\n",
    "+ $\\gamma$ is 0.9: $\\pi_{right}$ is optimal.\n",
    "+ $\\gamma$ is 0.5: both are optimal.\n",
    "\n",
    "#### 3.23\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    q_*(s, a) = \\sum_{s', r} p(s', r | s, a)[r + \\gamma \\max_{a'} q_*(s', a')]\n",
    "$$\n",
    "\n",
    "#### 3.24\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    \\gamma = \\frac{16.0}{17.8} = 0.9\\\\\n",
    "     24.4 = 10 + 16\\gamma\n",
    "$$\n",
    "\n",
    "#### 3.25\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    v_{*}(s) = \\sum_{a}\\pi^*(a|s)q_{*}(s,a)\n",
    "$$\n",
    "\n",
    "#### 3.26\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    q*(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{*}(s')]\n",
    "$$\n",
    "\n",
    "#### 3.27\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    \\pi_*(a|s) = \\mathcal{1}\\{a = argmax_{a'}q_*(s,a')\\}\n",
    "$$\n",
    "\n",
    "#### 3.28\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    \\pi_*(a|s) = \\mathcal{1}\\{a = argmax_{a'}\\sum_{s',r}p(s',r|s,a')[r+\\gamma v_{*}(s')]\\}\n",
    "$$\n",
    "\n",
    "#### 3.29\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    v_{\\pi}(s) &= \\sum_{a}\\pi(a|s)\\sum_{s'}p(s'|s,a)[r(s,a)+\\gamma v_{\\pi}(s')]\\\\\n",
    "    v_{*}(s) &= \\max_{a}\\sum_{s'}p(s'|s,a)[r(s,a)+\\gamma v_{*}(s')]\\\\\n",
    "    q_{\\pi}(s,a) &= \\sum_{s'}p(s'|s,a)[r(s,a)+\\gamma\\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a')] \\\\\n",
    "    q_{*}(s,a) &= \\sum_{s'}p(s'|s,a)[r(s,a)+\\gamma\\max_{a'}q_{*}(s',a')]\n",
    "    \\end{align*}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
