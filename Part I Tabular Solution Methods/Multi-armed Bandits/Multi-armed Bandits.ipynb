{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandits\n",
    "\n",
    "#### Difference between \"evaluative feedback\" and \"instructive feedback\"\n",
    "+ evaluative feedback can tell that how your result performs, compared with the best methods/parameters.\n",
    "+ instructive feedback can only originate a quality index of the result, but nothing to compare with it to know:\n",
    "     - whether it is the best result\n",
    "     - whether the result is good or not, and why is it good/bad, by methods/parameters or coincidence.\n",
    "     \n",
    "#### Simple Balancing Methods\n",
    "1. Greedy\n",
    "    - Stationary\n",
    "    $$\n",
    "        greedy \\to \\epsilon-greedy \\to computation efficient way(Incremental)\n",
    "    $$\n",
    "    $\\epsilon-greedy$ provides asymptotic guarantees of convergency of the value estimation.\n",
    "    - Constant Step-size for Nonstationary\n",
    "    \n",
    "    In such case it makes sense to give more weight to recent rewards than to long-past rewards, meaning that the step-size parameter is constant.\n",
    "\n",
    "2. Upper Confidence Bound\n",
    "    \\begin{equation}\n",
    "     A_t = {argmax}_{a}\\left[ \\, Q_t(a)+ c \\sqrt{\\frac{\\mathrm{ln}(t)}{N_t(a)}} \\, \\right]\n",
    "    \\end{equation}\n",
    "    \n",
    "    The idea is to use the uncertainty, which is measured by frequency.\n",
    "\n",
    "3. Gradient Bandit Algorithm\n",
    "    Using preference to distinguish actions, and update the preference after each choice.\n",
    "    The use of baseline will improve the performance.\n",
    "    \n",
    "#### Optimistic Initial Values\n",
    "Initial values can encourge exlporations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration and Exploitation\n",
    "\n",
    "#### Principles\n",
    "+ Naive Exploration\n",
    "+ Optimistic Initialisation\n",
    "    - simple and practical idea: initialise Q(a) to high value and N(a) > 0\n",
    "+ Optimism in the Face of Uncertainty\n",
    "    - explore the most uncertain action\n",
    "+ Probability Matching\n",
    "    - Thompson sampling\n",
    "+ Information State Search\n",
    "    - Information State Space\n",
    "\n",
    "Asymptotic total regret is at least logarithmic in number of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
