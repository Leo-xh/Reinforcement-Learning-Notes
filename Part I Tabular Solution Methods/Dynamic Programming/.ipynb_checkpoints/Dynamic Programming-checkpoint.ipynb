{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "#### General Concept\n",
    "DP refers to a collection of algorithms  that can be used to compute optimal policies given perfect environments, but they are computation-expensive and the requirement of a perfect environment adds limit to the praticability of DP.\n",
    "\n",
    "#### Policy Evaluation\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    v_{k+1} &\\doteq \\mathbb{E} [R_{t+1} + \\gamma v_{k}(S_{t+1}) | S_t = s] \\\\\n",
    "            &= \\sum_a \\pi(s|a) \\sum_{s', r} p(s', r| s, a) \\left[r + \\gamma v_k(s')\\right] \\\\\n",
    "    &\\text{iterative policy evaluation}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The in-place implementation should be faster.\n",
    "\n",
    "Pay attention that the convergence value may be the fixed point value or the solution of some linear system.\n",
    "\n",
    "#### Policy Improvement\n",
    "\n",
    "$$\n",
    "    \\pi' \\doteq arg\\max_{a}q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "#### Policy Iteration\n",
    "\n",
    "Combining **policy Evaluation** and **policy Improvement**, **policy Iteration** can monotonically improve policies and value functions. \n",
    "\n",
    "#### Value Iteration\n",
    "\n",
    "**Value Iteration** combines **policy Evaluation** and **policy Improvement**, thus it can convergence faster to the same result.The policy is extracted by \n",
    "$$\n",
    "    \\pi(s) = arg\\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma V(s')]\n",
    "$$\n",
    "\n",
    "#### Asynchronous DP\n",
    "flexibly select states to update, thus can update the states that are closely relavant to the running agent.\n",
    "\n",
    "#### DP is Quite Efficient\n",
    "Whereas, DP uses the full backup tree, which might be still expensive when is scale of problem is large, and sample DP can reduce the problem.\n",
    "\n",
    "#### Why Does DP Converge?\n",
    "For the value space is complete, and the Bellman equation is a Contraction Mapping, DP is  convergent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
