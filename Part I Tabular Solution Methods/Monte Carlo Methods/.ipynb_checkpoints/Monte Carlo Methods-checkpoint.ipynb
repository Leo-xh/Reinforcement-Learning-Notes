{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC\n",
    "\n",
    "#### Not Explicit Model Required\n",
    "Monte Carlo methods require only experience, although a model which is used to simulate the environment is needed.\n",
    "\n",
    "MC is defined on episodic tasks, thus the policy and value function are changed episodically.\n",
    "\n",
    "Because all action selections are undergoing learning, the returned reward depends on the actions taken in later states in the same episode.\n",
    "\n",
    "MC is based on the Law of Large Number.\n",
    "\n",
    "#### Prediction of $v_{\\pi}$ and $q_{\\pi}$\n",
    "\n",
    "+ First-visit MC\n",
    "+ Every-visit MC\n",
    "\n",
    "The methods above both converge quadratically.\n",
    "\n",
    "Esimation of state value is not sufficient without the model, but estimation of state-action value does.\n",
    "\n",
    "However, in the estimation of state-action value, some state-action pairs will never be visited. The ways to solve is:\n",
    "+ exploring starts\n",
    "+ consider stochastic policy.\n",
    "\n",
    "#### DP vs MC\n",
    "MC has 3 advantages\n",
    "+ the ability to learn from actual experience\n",
    "+ the ability to learn from simulated experience\n",
    "+ MC estimate states independently, thus it can update certain states, ignoring others.\n",
    "\n",
    "#### Monte Carlo Control\n",
    "Two strong original assumption:\n",
    "+ exploring starts\n",
    "+ infinite episodes\n",
    "\n",
    "**Removal of the two assumption to make this method more practical**\n",
    "+ exploring starts: ensure that different actions will be selected infinitely often.\n",
    "    - on-policy: greedy\n",
    "    - off-policy:\n",
    "    $\n",
    "        \\begin{cases}\n",
    "        \\text{target policy} \\\\\n",
    "        \\text{behavior policy}\n",
    "        \\end{cases}\n",
    "    $\n",
    "    \n",
    "    important sampling is used to correct the distribution over the behavior policy.\n",
    "    two different kinds of improved important sampling\n",
    "    + discounting-aware importance sampling: consider the discounting rate the probability of termination, aimed at reducing the variance.\n",
    "    + per-decision importance sampling: \n",
    "+ infinite episodes: \n",
    "    - using bounds\n",
    "    - give up trying to complete policy evaluation before returning t policy improvement\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
