{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD\n",
    "\n",
    "#### Overview\n",
    "Temporal difference learning is a combination of MC and DP, which is bootstrapping with raw experience.\n",
    "\n",
    "#### TD(0)\n",
    "The difference between MC and TD(0) is that tha latter only needs the next state, it uses $ V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1} - V(S_t)] $.\n",
    "Remind that MC use the theoritical basis that $v_{\\pi} = \\mathbb{E}_{\\pi}[G_t|S_{t=s}]$, while TD(0) use $v_{\\pi} = \\mathbb{E}_{\\pi}[G_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_{t=s}]$.\n",
    "\n",
    "Note that there is a TD error since TD(0) is using sample estimation but expectation estimation.\n",
    "\n",
    "#### Advantages of TD Prediction\n",
    "+ compared with DP, TD needs no model.\n",
    "+ compared with MC, TD only concerns about the next state, without the need to wait for the whole episode to finish, thus TD doesn't need the game to terminate, while MC does.\n",
    "+ TD is also guaranteed to converge is $\\alpha$ is small and satisfies the stochasitic approximation condition.\n",
    "\n",
    "#### Optimality of TD(0)\n",
    "+ TD converges to the maximum-likelyhood estimation of the value function. It is called certainty-equilvalent estimate because it is estimated by certainty rather than approximation, assuming that the process is under certainty.\n",
    "+ MC converges to the minimum mean-squared estimation.\n",
    "\n",
    "TD-estimated the value function is more similiar to the Markov process diagram than MC-estimated one.\n",
    "\n",
    "True TD Target $R_{t+1}+\\gamma v_{\\pi}(S_{t+1})$ is unbiased estimate, while TD target $R_{t+1}+\\gamma V(S_{t+1})$ is biased estimate, since $V(S_{end})  \\neq v_{\\pi}(S_{end})$\n",
    "\n",
    "TD target has much lower variance than $G_t$ since it depends only on one action, transition and reward.\n",
    "\n",
    "#### Sarsa\n",
    "On-policy learning.\n",
    "\n",
    "#### Q-learning\n",
    "Approximate directly $q_{*}$. Q-learning is off-policy because the value function update depends on a pure-greedy policy, instead of the policy being updated.\n",
    "\n",
    "#### Expected Sarsa\n",
    "Expected Sarsa can be used as off-policy method or on-policy method, depending on how the action is selected.\n",
    "\n",
    "It dominate other TD algorithms.\n",
    "\n",
    "#### Double Learning\n",
    "Using a single estimation to extraction the maximum action and update the estimation will raise maximizatin bias.\n",
    "\n",
    "Using two estimation leads to unbiased estimations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
