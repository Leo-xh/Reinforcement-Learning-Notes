{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "### Overview\n",
    "\n",
    "+ to ensure exploration, it is required that $\\pi(a|s,\\boldsymbol{\\theta}) \\in (0,1)$\n",
    "\n",
    "### Advantages\n",
    "\n",
    "+ it can approaches a deterministic policy\n",
    "+ it can approaches a stochastic policy\n",
    "+ it may be a simpler function with high probability\n",
    "+ it is easier to inject prior knowledge into the parameters\n",
    "\n",
    "### The Policy Gradient Theorem\n",
    "\n",
    "The theorem ensures the improvement of the function approximation, that is :\n",
    "$$\n",
    "    \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s}\\mu(s)\\sum_{a}\\nabla\\pi(a|s,\\boldsymbol{\\theta})q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "### baseline\n",
    "$$\n",
    "    \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s}\\mu(s)\\sum_{a}\\nabla\\pi(a|s,\\boldsymbol{\\theta})(q_{\\pi}(s,a)-b(s))\n",
    "$$\n",
    "An arbitrary baseline can reduce the variance, and thus the algorithm will convergence fast but leaving the expectation of the function unchanged.\n",
    "\n",
    "### Actor-Critic Methods\n",
    "use the learn value estimation to replace the true gain.\n",
    "\n",
    "this methods can reduct the variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
